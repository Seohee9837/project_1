import  requests
from bs4 import BeautifulSoup as bs
from tqdm import tqdm #í˜„ì¬ í¬ë¡¤ë§ ìƒí™© ì²´í¬
from fake_useragent import UserAgent
import datetime

def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        # "2025.06.25" í˜•ì‹ì„ íŒŒì‹±
        return datetime.datetime.strptime(date_str, "%Y.%m.%d")
    except ValueError:
        return None

def is_date_older_or_equal(current_date_str, target_date_str):
    """í˜„ì¬ ë‚ ì§œê°€ ëª©í‘œ ë‚ ì§œë³´ë‹¤ ê³¼ê±°ì´ê±°ë‚˜ ê°™ì€ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    current_date = parse_date(current_date_str)
    target_date = parse_date(target_date_str)
    
    if current_date is None or target_date is None:
        return False
    
    return current_date <= target_date

def dfinder(start_date, end_date, stock_code):
    find_first_date = False
    find_end_date = False
    crawled_url_4_date = []
    start_date_page = None
    end_date_page = None
    
    print(f"{start_date}ì—ì„œ {end_date} ë‚ ì§œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
    
    #í¬ë¡¤ëŸ¬
    for t in tqdm(range(1, 100084)):
        url = f'https://finance.naver.com/item/board.naver?code={stock_code}&page={t}'#ì ‘ì† ë§í¬
        user_agent = UserAgent()#fake_useragentí•¨ìˆ˜, useragent ë Œë¤ê°’ ì„¤ì •í•˜ê¸° ìœ„í•´ UserAgent()ë¶ˆëŸ¬ì˜´
        if(t <= 99): #99í˜ì´ì§€ ì „ê¹Œì§€ëŠ” ì°¸ì¡°í•  í•„ìš” ì—†ìŒ, ì˜¤íˆë ¤ ì°¸ì¡°í•˜ë©´ ì ‘ì† ë¶ˆê°€
            headers = {
                'User-Agent': user_agent.random #ëœë¤ agent ì„¤ì •
            }
        else: #1í˜ì´ì§€ ì´í›„ ì°¸ì¡°
            headers = {
                'Referer': f'https://finance.naver.com/item/board.naver?code={stock_code}&page={t-1}', #Refererë¥¼ ì´ìš©í•´ì„œ ë‹¤ìŒ ë§í¬ë¡œ ë“¤ì–´ê°€ê¸° ì „ì— ì°¸ì¡°, ì›ë˜ 100í˜ì´ì§€ ì´í›„ë¶€í„° ì°¸ì¡° í•„ìš”í•œë° êµ³ì´?
                'User-Agent' : user_agent.random #ëœë¤ agent ì„¤ì •
                }
        
        response = requests.get(url = url, headers = headers) #urlì ‘ì†
        #print(response) # ì ‘ì† í™•ì¸ (200 = ì„±ê³µ)
        html_text = response.text #responseì—ì„œ htmlê°’ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
        html = bs(html_text, 'html.parser') #beautifulsoupë¡œ HTML ë¬¸ì„œë¥¼ íŒŒì‹±
        
        # ë” êµ¬ì²´ì ì¸ ë‚ ì§œ ì„ íƒì ì‚¬ìš©
        date_crawling = html.select('span.tah.p10.gray03') #span íƒœê·¸ì™€ í•¨ê»˜ ëª…ì‹œ
        title_parsing = html.select('.title')

        # ì²« ë²ˆì§¸ í˜ì´ì§€ ë””ë²„ê¹…
        if t == 1:
            print(f"\nğŸ” ì²« ë²ˆì§¸ í˜ì´ì§€ ë””ë²„ê¹…:")
            print(f"URL: {url}")
            print(f"Response status: {response.status_code}")
            print(f"ë‚ ì§œ ìš”ì†Œ ê°œìˆ˜: {len(date_crawling)}")
            print(f"ì œëª© ìš”ì†Œ ê°œìˆ˜: {len(title_parsing)}")
            print("ë°œê²¬ëœ ëª¨ë“  ë‚ ì§œ:")
            for i, date_elem in enumerate(date_crawling[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                print(f"  {i+1}: '{date_elem.text.strip()}'")
            print("ë°œê²¬ëœ ëª¨ë“  ì œëª©:")
            for i, title_elem in enumerate(title_parsing[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                title_text = title_elem.text.strip() if title_elem.text else "ì œëª© ì—†ìŒ"
                print(f"  {i+1}: '{title_text}'")

        #.title í´ë˜ìŠ¤ì—ì„œ a.href ì†ì„±ê°’ë§Œ ì¶”ì¶œ
        # ì‹œì‘ ë‚ ì§œì™€ ë ë‚ ì§œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì°¾ê¸°
        for n in date_crawling:
            check = n.text.strip()
            # ë””ë²„ê¹…: ì²« ë²ˆì§¸ í˜ì´ì§€ì˜ ëª¨ë“  ë‚ ì§œ ì¶œë ¥
            if t == 1:
                print(f"í˜ì´ì§€ {t}ì—ì„œ ë°œê²¬í•œ ë‚ ì§œ: {check}")
            # ë””ë²„ê¹…: ì²« ëª‡ í˜ì´ì§€ì˜ ë‚ ì§œ ì¶œë ¥
            elif t <= 5:
                print(f"í˜ì´ì§€ {t}ì—ì„œ ë°œê²¬í•œ ë‚ ì§œ: {check}")
            
            # ì‹œì‘ ë‚ ì§œ ì°¾ê¸°
            if not find_first_date and start_date in check:
                start_date_page = t
                find_first_date = True
                print(f"\n{start_date}ë¥¼ {start_date_page}í˜ì´ì§€ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # ë ë‚ ì§œ ì°¾ê¸° - ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ ë” ê³¼ê±°ì¸ ë‚ ì§œê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
            if not find_end_date:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
                if end_date in check:
                    end_date_page = t
                    find_end_date = True
                    print(f"\n{end_date}ë¥¼ {end_date_page}í˜ì´ì§€ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                # ë” ê³¼ê±°ì¸ ë‚ ì§œê°€ ë‚˜ì˜¨ ê²½ìš°
                elif is_date_older_or_equal(check, end_date):
                    end_date_page = t
                    find_end_date = True
                    print(f"\n{check} (ëª©í‘œ: {end_date}ë³´ë‹¤ ê³¼ê±°)ë¥¼ {end_date_page}í˜ì´ì§€ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # URL ìˆ˜ì§‘ (ì‹œì‘ ë‚ ì§œë¥¼ ì°¾ì€ í›„ì—ë§Œ)
        if find_first_date and not find_end_date:
            for n in title_parsing:
                if n.a and n.a.get("href"):  # n.aê°€ ì¡´ì¬í•˜ê³  href ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
                    crawled_url_4_date.append(n.a["href"])
        
        # ë‘ ë‚ ì§œë¥¼ ëª¨ë‘ ì°¾ì•˜ìœ¼ë©´ ì¢…ë£Œ
        if find_first_date and find_end_date:
            break
            
        # 1000í˜ì´ì§€ê¹Œì§€ ê²€ìƒ‰í–ˆëŠ”ë° ëª» ì°¾ìœ¼ë©´ ì¢…ë£Œ
        if t >= 1000:
            print(f"\nâš ï¸ 1000í˜ì´ì§€ê¹Œì§€ ê²€ìƒ‰í–ˆì§€ë§Œ ëª¨ë“  ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            print(f"ì‹œì‘ ë‚ ì§œ ({start_date}) ì°¾ìŒ: {find_first_date}")
            print(f"ë ë‚ ì§œ ({end_date}) ì°¾ìŒ: {find_end_date}")
            break
                
    return start_date_page, end_date_page, crawled_url_4_date #ë‚ ì§œì— ë§ëŠ” í˜ì´ì§€ ë¦¬í„´


if __name__ == "__main__":
    start, end, url = dfinder("2023.12.11", "2023.12.10", "005930")
    print(start, end, url)