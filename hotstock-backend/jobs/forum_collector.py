from datetime import datetime, timedelta
import os
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup as bs
from tqdm import tqdm
from collections import Counter
from openai import OpenAI
from fake_useragent import UserAgent
from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
from dotenv import load_dotenv

load_dotenv()

# ë‚ ì§œ ìë™ ì„¤ì •: ì–´ì œ ~ ê·¸ì €ê»˜
today = datetime.now()
start_date = (today - timedelta(days=1)).strftime('%Y.%m.%d')
end_date = (today - timedelta(days=2)).strftime('%Y.%m.%d')
# GPT API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ê²Œì‹œê¸€ URL ìˆ˜ì§‘ê¸°
def dfinder(start_date, end_date, stock_code):
    """
    âœ… start_date ~ end_dateì— í•´ë‹¹í•˜ëŠ” ê²Œì‹œê¸€ URLë§Œ ìˆ˜ì§‘
    âœ… í˜ì´ì§€ ë‚´ì—ì„œ end_dateë³´ë‹¤ ì´ì „ ë‚ ì§œê°€ ë“±ì¥í•˜ë©´ í•´ë‹¹ í˜ì´ì§€ê¹Œì§€ë§Œ ìˆ˜ì§‘í•˜ê³  ì¢…ë£Œ
    """
    crawled_urls = []
    dt_start = datetime.strptime(start_date, "%Y.%m.%d")
    dt_end = datetime.strptime(end_date, "%Y.%m.%d")

    for t in tqdm(range(1, 100084)):
        url = f'https://finance.naver.com/item/board.naver?code={stock_code}&page={t}'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)
            html = bs(response.text, 'html.parser')
            date_elements = html.select('.tah.p10.gray03')
            title_elements = html.select('.title')

            page_has_valid_date = False
            should_stop = False

            for n in date_elements:
                match = re.search(r'\d{4}\.\d{2}\.\d{2}', n.text)
                if not match:
                    continue
                post_date = datetime.strptime(match.group(), "%Y.%m.%d")

                if dt_end <= post_date <= dt_start:
                    page_has_valid_date = True
                elif post_date < dt_end:
                    should_stop = True

            # âœ… ìˆ˜ì§‘ì€ 29ì¼ ê²Œì‹œê¸€ë§Œ (í˜¹ì€ ë²”ìœ„ ë‚´ ê²Œì‹œê¸€ë§Œ)
            if page_has_valid_date:
                for n in title_elements:
                    if n.a and n.a.get("href"):
                        href = n.a["href"]
                        if href.startswith("/item/board_read"):
                            crawled_urls.append("https://finance.naver.com" + href)

            if should_stop:
                print(f"ğŸ›‘ {stock_code}: {end_date} ì´ì „ ë‚ ì§œ ë°œê²¬ â†’ ì¢…ë£Œ.")
                break

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {t} ì—ëŸ¬: {e}")
            continue

    return crawled_urls

# ê²Œì‹œê¸€ ë³¸ë¬¸/ì œëª©/ë‚ ì§œ ìˆ˜ì§‘ê¸°
def CWBS(url_list, stock_code, target_date):
    date_list, title_list, content_list = [], [], []
    for url in tqdm(url_list):
        try:
            user_agent = UserAgent()
            headers = {
                'Referer': f'https://finance.naver.com/item/board.naver?code={stock_code}',
                'User-Agent': user_agent.random
            }
            response = requests.get(url=url, headers=headers)
            html = bs(response.text, 'html.parser')

            date = html.select_one('.gray03.p9.tah')
            title = html.select_one('.c.p15')
            content = ""
            for selector in ['.view_se', '.se-main-container', '.se-component-content', '.post_content']:
                content_element = html.select_one(selector)
                if content_element:
                    content = content_element.get_text(strip=True)
                    break
            if not (date and title and content):
                continue

            date_text = date.text[:10]

            if date_text != target_date:  # âœ… 29ì¼ì´ ì•„ë‹Œ ê²Œì‹œê¸€ì€ ì œì™¸
                continue

            title_text = ILLEGAL_CHARACTERS_RE.sub(r'', title.text.replace("#", "").replace("[", "").replace("]", ""))
            content_text = ILLEGAL_CHARACTERS_RE.sub(r'', content.replace("\n", " ").replace("\r", " ").replace("\t", " "))
            date_list.append(date_text)
            title_list.append(title_text)
            content_list.append(content_text)
        except:
            continue
    return pd.DataFrame({'date': date_list, 'title': title_list, 'content': content_list})

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords_from_text(text):
    text = re.sub(r'\s+', ' ', text)
    korean_words = re.findall(r'[ê°€-í£]{2,}', text)
    english_words = re.findall(r'\b[a-zA-Z]{3,}\b', text)
    number_korean = re.findall(r'\d+[ê°€-í£]+', text)
    special_patterns = re.findall(r'[ê°€-í£]+(?:ë¥ |ì|ì„±|ë ¥|ê°|ë„|í˜•|ì |í™”)', text)
    return korean_words + english_words + number_korean + special_patterns

# GPT í•„í„°ë§
def filter_keywords_with_gpt(top_keywords, stock_code):
    keyword_list = [f"{word}({freq}íšŒ)" for word, freq in top_keywords]
    keyword_str = ", ".join(keyword_list)
    prompt = f"""
ë‹¤ìŒì€ {stock_code} ì¢…ëª© ê´€ë ¨ ì¢…ëª©í† ë¡ ë°© ê²Œì‹œê¸€ì—ì„œ ì¶”ì¶œí•œ ìƒìœ„ í‚¤ì›Œë“œ ëª©ë¡ì…ë‹ˆë‹¤. 
ì¢…ëª©í† ë¡ ë°©ì˜ ìƒìƒí•œ ì—¬ë¡ ì„ ì „ë‹¬í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì„ ë³„í•´ì£¼ì„¸ìš”(ìƒìœ„ 20ê°œ):

âœ… í¬í•¨í•´ì•¼ í•  ê²ƒë“¤:
- ë¹„ì†ì–´, ì¸í„°ë„·ìš©ì–´, ì‹ ì¡°ì–´
- ë‹¤ë¥¸ ê¸°ì—…ëª…
- ì£¼ì‹ ê´€ë ¨ ì „ë¬¸ìš©ì–´
- íˆ¬ìì ê°ì • í‘œí˜„
- ì‹œì¥ ë™í–¥ í‚¤ì›Œë“œ
- ì‹¤ì , ë‰´ìŠ¤, ì´ë²¤íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œ

âŒ ì œê±°í•´ì•¼ í•  ê²ƒë“¤:
- ì¡°ì‚¬, ì ‘ì†ì‚¬, ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬
- ì‹œê°„ í‘œí˜„
- ì •ë„ í‘œí˜„
- í•´ë‹¹ ê¸°ì—…ëª…

í‚¤ì›Œë“œ ëª©ë¡: {keyword_str}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
1. í‚¤ì›Œë“œëª…(íšŸìˆ˜íšŒ)
2. í‚¤ì›Œë“œëª…(íšŸìˆ˜íšŒ)
...
"""
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¢…ëª©í† ë¡ ë°© ì—¬ë¡  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("GPT ì˜¤ë¥˜:", e)
        return ""

# ë¶„ì„ ì‹¤í–‰
def run_analysis(stock_code):
    print(f"ğŸ“Œ ì‹¤í–‰ ë‚ ì§œ ê¸°ì¤€: start_date={start_date}, end_date={end_date}")
    url_list = dfinder(start_date, end_date, stock_code)  # ğŸ” end_dateë„ ì „ë‹¬
    if not url_list:
        print("âŒ URL ì—†ìŒ")
        return

    df = CWBS(url_list, stock_code, start_date)  # âœ… start_date ê¸°ì¤€ í•„í„°ë§ ìœ ì§€
    if df.empty:
        print("âŒ í•´ë‹¹ ë‚ ì§œ ê²Œì‹œê¸€ ì—†ìŒ")
        return

    all_text = " ".join(df['title'] + " " + df['content'])
    all_words = extract_keywords_from_text(all_text)
    top_words = Counter(all_words).most_common(100)
    filtered = filter_keywords_with_gpt(top_words[:50], stock_code)

    keywords = re.findall(r'\d+\.\s*(.+?\(\d+íšŒ\))', filtered)
    keywords = keywords[:20]

    result_df = pd.DataFrame([[stock_code] + keywords], columns=['ticker'] + [f"keyword{i+1}" for i in range(len(keywords))])
    return result_df

def collect_forum_keywords(limit=3):
    try:
        ticker_df = pd.read_csv("../data/2024_final_ticker_list.csv", dtype={"ticker": str})
        if 'ticker' not in ticker_df.columns:
            raise ValueError("âŒ 'ticker' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤.")
        test_tickers = ticker_df['ticker'].dropna().unique().tolist()
        print("ğŸ“„ ì¶”ì¶œëœ í‹°ì»¤ ëª©ë¡ (ì¼ë¶€):", test_tickers[:limit])
    except Exception as e:
        print(f"âŒ CSV ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

    final_df = pd.DataFrame()
    tried = 0

    for ticker in test_tickers:
        if limit is not None and tried >= limit:
            break
        tried += 1
        try:
            print(f"\nğŸš€ {ticker} ì²˜ë¦¬ ì‹œì‘")
            result_df = run_analysis(ticker)
            if result_df is not None:
                final_df = pd.concat([final_df, result_df], ignore_index=True)
        except Exception as e:
            print(f"âŒ {ticker} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    if not final_df.empty:
        os.makedirs("data", exist_ok=True)
        final_df.to_csv("../data/forums.csv", index=False, encoding="utf-8-sig")
        print("\nâœ… ëˆ„ì  ê²°ê³¼ ì €ì¥ ì™„ë£Œ: data/forums.csv")
        return final_df
    else:
        print("âš ï¸ ê²°ê³¼ê°€ ì—†ì–´ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

