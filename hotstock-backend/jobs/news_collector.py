import os
import re
import requests
import pandas as pd
from newspaper import Article
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
from collections import Counter
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ğŸ“‚ ìƒëŒ€ ê²½ë¡œ (news_collector.py ê¸°ì¤€)
CSV_PATH = "../../2024_final_ticker_list.csv"
OUTPUT_PATH = "../data/news.csv"

# ğŸ“ ëª¨ë“  í‹°ì»¤ ë¶ˆëŸ¬ì˜¤ê¸°
def load_all_tickers(csv_path=CSV_PATH):
    try:
        df = pd.read_csv(csv_path, dtype=str)
        return list(df[["ticker", "corp_name"]].dropna().itertuples(index=False, name=None))
    except Exception as e:
        print(f"[ERROR] CSV ë¡œë”© ì‹¤íŒ¨: {e}")
        return []

# ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
def search_naver_news_all(query, target_dates):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    all_filtered = []
    for start in range(1, 500, 100):
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "sim"
        }
        try:
            response = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params)
            response.raise_for_status()
        except:
            break
        items = response.json().get("items", [])
        for item in items:
            try:
                pubdate = parsedate_to_datetime(item["pubDate"]).date()
                if pubdate in target_dates:
                    all_filtered.append(item["originallink"].replace("amp;", ""))
            except:
                continue
    return all_filtered

# ğŸ“„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
def extract_article_text(url):
    try:
        article = Article(url, language="ko")
        article.download()
        article.parse()
        return article.text
    except:
        return None

# ğŸ·ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text):
    return re.findall(r'\b[ê°€-í£]{2,}\b', text)

# ğŸ¤– GPT í•„í„°ë§
def filter_keywords_with_gpt(top_keywords, company_name):
    keyword_list = [f"{word}({freq})" for word, freq in top_keywords if word != company_name]
    keyword_str = ", ".join(keyword_list)

    prompt = f"""
ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ìƒìœ„ í‚¤ì›Œë“œ ëª©ë¡ì…ë‹ˆë‹¤. ì´ ì¤‘ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ëŠ” ì „ë¶€ ì œê±°í•˜ê³ , í•µì‹¬ ëª…ì‚¬ í‚¤ì›Œë“œë§Œ ë‚¨ê²¨ì£¼ì„¸ìš”.

- ê¸°ì—… ì´ë¦„ ìì²´
- ì¡°ì‚¬(ì´, ê°€, ì„, ë¥¼, ì€, ëŠ” ë“±)
- ë¶ˆìš©ì–´, ì–´ë¯¸ê°€ ë¶™ì€ ë‹¨ì–´ (ì˜ˆ: ê°ˆë“±ì´ â†’ ê°ˆë“±)
- ì¼ë°˜ì ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ (ì˜ˆ: ê´€ë ¨, ì§„í–‰, ìƒí™© ë“±)
- ì˜ë¯¸ê°€ ì¤‘ë³µë˜ê±°ë‚˜ ë„ˆë¬´ ì¶”ìƒì ì¸ ë‹¨ì–´

ìµœì¢…ì ìœ¼ë¡œëŠ” **ëª…ì‚¬í˜• í•µì‹¬ í‚¤ì›Œë“œë§Œ** ë‚¨ê¸°ê³ , í˜•íƒœì†Œê°€ ì„ì¸ ë‹¨ì–´ëŠ” ì›í˜•ìœ¼ë¡œ ì •ì œí•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒì²˜ëŸ¼ í•´ì£¼ì„¸ìš”:
í‚¤ì›Œë“œ1(íšŸìˆ˜), í‚¤ì›Œë“œ2(íšŸìˆ˜), í‚¤ì›Œë“œ3(íšŸìˆ˜), ...

ë‹¤ìŒì€ í‚¤ì›Œë“œì™€ ì–¸ê¸‰íšŸìˆ˜ì…ë‹ˆë‹¤: {keyword_str}

"""
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ í‚¤ì›Œë“œ í•„í„°ë§ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[ERROR] GPT ì‘ë‹µ ì˜¤ë¥˜: {e}")
        return ""

# ğŸ¯ GPT ì‘ë‹µ íŒŒì‹±
def parse_filtered_keywords(filtered_str):
    pattern = re.compile(r'([^,()\s]+)\((\d+)\)')
    return [(m.group(1), int(m.group(2))) for m in pattern.finditer(filtered_str)]

# ğŸ” ìµœê·¼ ë‰´ìŠ¤ ìˆ˜ì§‘
def get_recent_articles(company_name, max_days=7):
    today = datetime.now().date()
    for i in range(max_days):
        check_date = today - timedelta(days=i+1)
        articles = search_naver_news_all(company_name, [check_date])
        if articles:
            return articles
    return []

# âœ… ë‹¨ì¼ í‹°ì»¤ ì²˜ë¦¬
def process_ticker(ticker, corp_name):
    urls = get_recent_articles(corp_name)
    if not urls:
        print(f"[SKIP] ë‰´ìŠ¤ ì—†ìŒ: {corp_name}")
        return None

    full_text = ""
    for url in urls:
        text = extract_article_text(url)
        if text:
            full_text += text + "\n"

    keywords = extract_keywords(full_text)
    keyword_freq = Counter(keywords).most_common(30)

    filtered = filter_keywords_with_gpt(keyword_freq, corp_name)
    filtered_keywords = parse_filtered_keywords(filtered)

    # ë¬´ì¡°ê±´ 20ê°œ ì±„ìš°ê¸°
    row = {"ticker": ticker, "corp_name": corp_name}
    for i in range(20):
        if i < len(filtered_keywords):
            word, freq = filtered_keywords[i]
            row[f"keyword{i+1}"] = f"{word}({freq})"
        else:
            row[f"keyword{i+1}"] = ""
    return row

# âœ… ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
def collect_all_keywords(limit=10):  # limit=5ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥  ------------------> ì—¬ê¸° limit=5
    tickers = load_all_tickers()[:limit]  # ìƒìœ„ Nê°œ í‹°ì»¤ë§Œ  ------------------> ì—¬ê¸° [:limit]
    results = []

    for ticker, corp_name in tickers:
        print(f"[ì§„í–‰ì¤‘] {corp_name} ({ticker})")
        row = process_ticker(ticker, corp_name)
        if row:
            results.append(row)

    if results:
        df = pd.DataFrame(results)
        os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
        df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")
        print(f"[ì™„ë£Œ] {len(results)}ê°œ ê¸°ì—… ì €ì¥ â†’ {OUTPUT_PATH}")
    else:
        print("[ì•Œë¦¼] ì €ì¥í•  ë°ì´í„° ì—†ìŒ")

# ğŸš€ ì‹¤í–‰
if __name__ == "__main__":
    collect_all_keywords(limit=10)  # ------> ì§€ê¸ˆì€ ë˜ë‚˜ í™•ì¸ìš© ë‚˜ì¤‘ì— limit=5 ê¹Œì§€ 3ê°œê°œì§€ìš°ë©´ ë¨ ~ã……~
